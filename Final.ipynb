{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cdf69-10d7-4549-8270-442654acc877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62218e2f-db22-4e49-b70a-58a7859826e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c299bc-0bf9-474b-8be1-3112bf52647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output saved to structured_output.csv\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def parse_log_file(file_path):\n",
    "    # Reading the log file content\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "    \n",
    "    logs = []\n",
    "    for line in content:\n",
    "        timestamp, message = None, None\n",
    "        \n",
    "        # Format 1: ASA logs\n",
    "        asa_pattern = re.compile(r'(\\w+ \\d+ \\d{4} \\d{2}:\\d{2}:\\d{2}): (.+)')\n",
    "        match = asa_pattern.match(line)\n",
    "        if match:\n",
    "            timestamp, message = match.groups()\n",
    "            message_id_match = re.search(r'%ASA-\\d+-\\d+:', message)\n",
    "            message_id = message_id_match.group(0) if message_id_match else \"Unknown\"\n",
    "            readable_message = message.replace(message_id, \"\").strip()\n",
    "        \n",
    "        # Format 2: Generic logs\n",
    "        generic_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}), (Info|Warning|Error)\\s*(.+)')\n",
    "        match = generic_pattern.match(line)\n",
    "        if match:\n",
    "            timestamp, level, message = match.groups()\n",
    "            message_id = \"Unknown\"\n",
    "            readable_message = message.strip()\n",
    "        \n",
    "        if timestamp and message:\n",
    "            logs.append({'timestamp': timestamp, 'message_id': message_id, 'readable_message': readable_message})\n",
    "    \n",
    "    if not logs:\n",
    "        raise ValueError(\"No valid log entries found. Please check the log file format.\")\n",
    "    \n",
    "    return pd.DataFrame(logs)\n",
    "\n",
    "def process_csv(file_path, tokenizer):\n",
    "    try:\n",
    "        df = parse_log_file(file_path)\n",
    "        df['tokens'] = df['readable_message'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Error processing CSV file: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while processing the CSV file: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def infer(model, tokenizer, df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = df['tokens'].tolist()\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(t) for t in inputs], batch_first=True, padding_value=0)\n",
    "    inputs_padded = inputs_padded.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs_padded)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().tolist()\n",
    "    \n",
    "    df['category'] = preds\n",
    "    return df\n",
    "\n",
    "def human_readable_output(df):\n",
    "    df['category'] = df['category'].apply(lambda x: 'Error' if x == 1 else 'Non-Error')\n",
    "    return df[['timestamp', 'message_id', 'readable_message', 'category']]\n",
    "\n",
    "def main(csv_file=None, model_path=None):\n",
    "    if csv_file and model_path:\n",
    "        try:\n",
    "            model, tokenizer = load_model(model_path)\n",
    "            df = process_csv(csv_file, tokenizer)\n",
    "            df = infer(model, tokenizer, df)\n",
    "            readable_df = human_readable_output(df)\n",
    "            \n",
    "            # Save the structured output to a CSV file\n",
    "            output_csv_path = 'structured_output.csv'\n",
    "            readable_df.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Structured output saved to {output_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"Log Categorization Script\")\n",
    "        parser.add_argument('csv_file', type=str, help='Path to the CSV file containing logs')\n",
    "        parser.add_argument('model_path', type=str, help='Path to the pre-trained model')\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        try:\n",
    "            model, tokenizer = load_model(args.model_path)\n",
    "            df = process_csv(args.csv_file, tokenizer)\n",
    "            df = infer(model, tokenizer, df)\n",
    "            readable_df = human_readable_output(df)\n",
    "            \n",
    "            # Save the structured output to a CSV file\n",
    "            output_csv_path = 'structured_output.csv'\n",
    "            readable_df.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Structured output saved to {output_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Define default paths for interactive environment\n",
    "csv_file_path = 'cisco_log.txt'  # Update this path based on the new log file\n",
    "model_path = '/Users/nithinrajulapati/Downloads/LLM for Logging/trained_model_from_SCRATCH'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # This is while Running in an interactive environment\n",
    "        main(csv_file=csv_file_path, model_path=model_path)\n",
    "    else:\n",
    "        # this Running from the command line\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ce605-24bf-407d-9346-ab9c7bece1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
