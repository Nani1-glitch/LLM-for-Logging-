{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbabbe95-b5c6-49f7-98f0-307dbbd42855",
   "metadata": {},
   "source": [
    "# Now Developing the LLM to analyze the file and produce the human readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edc27f9-2e26-4d4b-b196-068d01b8ff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'structured_logs.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the log file\n",
    "log_file_path = 'cisco_log.txt'\n",
    "\n",
    "# Define a function to parse the log entries\n",
    "def parse_log(log_content):\n",
    "    log_entries = []\n",
    "    log_pattern = re.compile(r'(?P<date>\\w+ +\\d+ \\d+:\\d+:\\d+).+:(?P<message>.+)')\n",
    "    for line in log_content.split('\\n'):\n",
    "        if line.strip():\n",
    "            match = log_pattern.match(line)\n",
    "            if match:\n",
    "                log_entries.append(match.groupdict())\n",
    "            else:\n",
    "                # Handle cases where the line does not match the expected format\n",
    "                log_entries.append({'date': None, 'message': line.strip()})\n",
    "    return log_entries\n",
    "\n",
    "# Read and parse the log file\n",
    "with open(log_file_path, 'r') as file:\n",
    "    log_content = file.read()\n",
    "    log_entries = parse_log(log_content)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df_logs = pd.DataFrame(log_entries)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'structured_logs.csv'\n",
    "df_logs.to_csv(csv_file_path, index=False)\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ce8e-4d41-4041-b444-0e08587cca5d",
   "metadata": {},
   "source": [
    "# Parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3daee0-dc81-4478-917b-47d4df192236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{\\rtf1\\ansi\\ansicpg1252\\cocoartf2761</td>\n",
       "      <td>[101, 1063, 1032, 19387, 2546, 2487, 1032, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\...</td>\n",
       "      <td>[101, 1032, 22940, 18209, 15782, 2989, 2692, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{\\colortbl;\\red255\\green255\\blue255;}</td>\n",
       "      <td>[101, 1063, 1032, 3609, 2102, 16558, 1025, 103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{\\*\\expandedcolortbl;;}</td>\n",
       "      <td>[101, 1063, 1032, 1008, 1032, 4423, 18717, 210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\margl1440\\margr1440\\vieww11520\\viewh8400\\view...</td>\n",
       "      <td>[101, 1032, 9388, 23296, 16932, 12740, 1032, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                                            message  \\\n",
       "0   NaN               {\\rtf1\\ansi\\ansicpg1252\\cocoartf2761   \n",
       "1   NaN  \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\...   \n",
       "2   NaN              {\\colortbl;\\red255\\green255\\blue255;}   \n",
       "3   NaN                            {\\*\\expandedcolortbl;;}   \n",
       "4   NaN  \\margl1440\\margr1440\\vieww11520\\viewh8400\\view...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [101, 1063, 1032, 19387, 2546, 2487, 1032, 201...  \n",
       "1  [101, 1032, 22940, 18209, 15782, 2989, 2692, 1...  \n",
       "2  [101, 1063, 1032, 3609, 2102, 16558, 1025, 103...  \n",
       "3  [101, 1063, 1032, 1008, 1032, 4423, 18717, 210...  \n",
       "4  [101, 1032, 9388, 23296, 16932, 12740, 1032, 9...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the structured log file\n",
    "csv_file_path = 'structured_logs.csv'\n",
    "df_logs = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the log messages\n",
    "df_logs['tokens'] = df_logs['message'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "df_logs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799105b-69cc-41e7-9ae4-ee319041feed",
   "metadata": {},
   "source": [
    "# Initializing and training the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a59477c-cba9-4c97-a7a2-6e55c6bfa996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.1657, -0.0562, -1.1300,  ..., -0.0024,  0.0442, -1.2355],\n",
      "         [-0.3658,  0.1583, -0.5717,  ...,  0.6249, -1.3246,  0.5423],\n",
      "         [-0.1087,  0.8095, -0.8744,  ...,  0.3942, -1.1411, -0.5234],\n",
      "         ...,\n",
      "         [ 1.1914,  0.3439, -1.5971,  ...,  0.4967, -0.1702, -0.6801],\n",
      "         [ 0.3705,  0.2289, -0.6759,  ..., -0.1672, -1.6718, -1.7447],\n",
      "         [ 0.9166,  0.9594, -1.5302,  ...,  0.9869,  0.0999, -1.5372]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-2.9979e-01, -1.5988e-01, -7.4318e-01,  1.5011e-01,  3.7035e-01,\n",
      "          6.7062e-01,  8.5490e-02, -3.8273e-01, -1.6664e-01, -1.8253e-01,\n",
      "         -3.7210e-01, -3.1502e-01,  5.0265e-01,  4.0950e-02, -1.7972e-01,\n",
      "          3.0770e-01, -4.7946e-01,  8.0233e-02, -6.9517e-01,  2.7487e-01,\n",
      "         -5.0945e-01, -4.3295e-01, -7.1375e-01, -8.8962e-01, -4.1633e-01,\n",
      "          2.6902e-01, -5.2506e-01,  4.4854e-02,  2.4527e-01, -3.9572e-01,\n",
      "          3.1679e-01, -4.0785e-01,  1.1756e-01,  1.1597e-02,  6.1851e-01,\n",
      "          8.3480e-01, -4.8492e-01,  3.7576e-01, -4.2127e-01,  1.3847e-01,\n",
      "         -3.9899e-02,  4.6962e-01,  3.7109e-01, -4.7198e-01, -2.1967e-01,\n",
      "         -1.3027e-01, -6.8814e-01, -5.2630e-01, -1.0358e-01, -6.4888e-01,\n",
      "         -1.3084e-01, -4.3138e-01,  3.5390e-01,  6.6159e-01, -2.9479e-01,\n",
      "          2.1855e-01, -3.5225e-01,  2.1330e-01,  7.5757e-01, -7.8161e-01,\n",
      "         -4.3714e-01,  6.7328e-01,  5.1021e-01,  3.5772e-01, -6.1505e-01,\n",
      "         -1.2204e-01, -1.8210e-01,  3.3632e-01, -4.4661e-01,  9.9103e-02,\n",
      "          7.2779e-01, -5.1207e-01, -6.3168e-01, -4.1745e-01,  4.0848e-03,\n",
      "         -4.0624e-01, -2.5572e-02,  4.8647e-01,  7.0962e-01,  2.7649e-01,\n",
      "          6.8362e-01,  4.5210e-01,  3.7633e-01,  5.2374e-01,  2.1455e-01,\n",
      "         -8.3869e-02, -2.5829e-01,  4.7709e-03,  2.1670e-01, -4.9475e-01,\n",
      "          2.0499e-01,  4.9675e-01, -6.4200e-01,  8.6220e-02, -3.1142e-01,\n",
      "          2.0097e-01,  6.3764e-01,  6.4384e-01, -5.3086e-02, -7.1214e-01,\n",
      "          2.3426e-01, -2.6178e-01,  2.4151e-01, -4.4348e-01, -8.0930e-01,\n",
      "         -1.3895e-02, -3.8626e-01, -2.9055e-01, -1.3349e-01, -7.3798e-01,\n",
      "         -7.7570e-02, -6.2883e-01, -1.2497e-01,  2.7931e-01,  3.9577e-01,\n",
      "         -6.0140e-01,  3.2514e-01,  8.8251e-01, -2.2838e-02,  2.1104e-01,\n",
      "         -5.5318e-01,  5.7558e-01, -5.9035e-01,  7.2994e-01, -9.3231e-02,\n",
      "          3.8852e-01,  2.4737e-01, -4.8059e-01, -7.8077e-01, -3.3197e-01,\n",
      "         -2.4971e-01, -7.2047e-02,  1.2283e-01,  2.4614e-01,  1.0649e-01,\n",
      "          4.0708e-01, -4.8349e-01,  4.6020e-01,  3.6833e-01, -7.2098e-01,\n",
      "          3.2800e-01, -2.7860e-01,  4.9617e-01,  7.0826e-01, -5.2189e-01,\n",
      "          2.9868e-01,  6.3482e-01,  5.5824e-01, -5.2372e-01,  4.7033e-01,\n",
      "          6.6930e-01, -1.1050e-01,  4.6953e-01, -6.9898e-01, -3.1272e-01,\n",
      "         -6.6998e-01,  6.6304e-01, -4.0687e-02, -1.7109e-01, -4.5028e-01,\n",
      "         -7.4332e-01,  4.0624e-01,  9.9349e-02, -7.5868e-01, -8.6305e-01,\n",
      "         -6.5035e-01,  5.0953e-01,  3.1631e-01, -8.1738e-02, -8.4280e-02,\n",
      "          7.7568e-01,  8.4729e-01,  8.1590e-01,  5.3489e-01, -5.2174e-02,\n",
      "          5.6160e-01, -5.8947e-01, -1.8564e-01,  5.1449e-01,  4.4378e-01,\n",
      "          1.2727e-01,  7.0904e-01,  4.3891e-01, -8.8907e-01,  2.9885e-02,\n",
      "         -5.2945e-01,  3.6531e-01, -4.6213e-01, -7.2947e-02, -8.8562e-01,\n",
      "         -8.5184e-01, -1.5104e-01,  5.6825e-01, -9.2818e-01, -9.4611e-02,\n",
      "         -7.4235e-01, -6.1423e-02,  5.5078e-01,  4.3574e-01, -3.8451e-01,\n",
      "          4.4846e-01,  4.4216e-01, -1.1407e-01,  8.4180e-01,  5.8760e-01,\n",
      "         -1.3352e-01, -2.4856e-01, -4.5711e-01,  7.8902e-01,  1.9754e-01,\n",
      "         -2.6053e-01, -3.9994e-01, -6.3030e-01,  8.8758e-01, -6.2530e-02,\n",
      "          2.3030e-01,  6.8829e-01,  1.2859e-01,  7.9684e-02, -3.5502e-01,\n",
      "          6.7989e-01,  3.2040e-01,  4.4781e-01,  6.2609e-01,  5.7682e-01,\n",
      "         -1.9506e-01,  5.2356e-01,  3.2384e-01,  5.8986e-01, -6.2803e-01,\n",
      "         -9.1292e-01,  4.8498e-01, -9.2574e-01, -2.0928e-01,  5.4180e-01,\n",
      "          9.3653e-02, -8.1116e-01, -8.7016e-01, -5.1098e-01, -9.1587e-02,\n",
      "          2.5975e-01, -1.5634e-01,  3.3195e-01, -1.0997e-01,  4.0068e-01,\n",
      "         -1.6327e-01, -1.6707e-01,  2.6963e-01,  2.7834e-01,  3.5390e-01,\n",
      "         -2.7004e-01,  1.7481e-01, -1.9119e-01,  1.6294e-01,  1.1575e-01,\n",
      "         -4.9194e-01, -2.4206e-01,  3.6399e-01, -4.1829e-01,  4.7250e-02,\n",
      "          6.8021e-01,  2.4194e-01,  2.8786e-01, -5.4721e-01,  3.5128e-01,\n",
      "         -5.5132e-01, -3.7646e-01,  3.5442e-01, -7.2434e-01, -2.7321e-01,\n",
      "          2.9579e-01,  3.4223e-01, -2.1142e-01, -3.4630e-01, -8.7546e-01,\n",
      "          9.8367e-02, -4.2351e-01,  4.8058e-01,  5.2100e-01, -5.4742e-01,\n",
      "         -5.1949e-01,  1.5385e-01, -4.8209e-01, -2.2115e-01,  2.8880e-01,\n",
      "          3.0466e-03, -6.0771e-01, -2.3113e-02, -5.1566e-01,  5.2019e-01,\n",
      "          4.7294e-01, -3.5677e-01, -5.0695e-01, -8.3738e-01, -3.4579e-01,\n",
      "         -1.6226e-01, -5.5069e-01, -2.2209e-01, -6.2280e-02, -6.8360e-03,\n",
      "         -7.9860e-01, -3.7940e-01,  2.3885e-01, -6.8489e-01, -2.1240e-01,\n",
      "         -4.7054e-01,  4.4691e-01,  2.9574e-01, -5.0516e-01, -8.5644e-01,\n",
      "          2.3814e-01, -6.1226e-02,  7.4398e-01, -5.2170e-01, -1.7698e-01,\n",
      "          7.1993e-01,  4.5879e-01, -5.7521e-01, -2.6115e-02,  7.6625e-01,\n",
      "         -2.3366e-01,  5.0486e-01,  1.3516e-01,  2.2828e-01, -1.8116e-01,\n",
      "          6.0179e-02, -1.5394e-01, -7.1536e-01, -3.7044e-01, -3.0790e-01,\n",
      "         -5.9530e-01, -2.2754e-01, -1.5048e-01,  1.0198e-01,  2.4970e-01,\n",
      "         -1.0483e-01, -8.3393e-02, -4.5624e-02, -7.5380e-02,  1.8007e-01,\n",
      "         -6.0572e-01,  5.4198e-01,  8.7986e-01, -2.1485e-01,  6.3352e-04,\n",
      "          9.6608e-02, -7.8534e-01,  2.1124e-02,  1.0923e-01, -3.5372e-01,\n",
      "          1.3838e-01,  1.9780e-01, -7.9846e-01,  3.0864e-01,  1.8521e-01,\n",
      "         -4.4604e-01,  7.3611e-01,  4.4645e-01,  1.5886e-01, -3.5346e-01,\n",
      "         -4.5143e-01, -1.4682e-01, -1.8666e-01,  4.4562e-01,  1.2151e-01,\n",
      "          1.7208e-01,  2.1995e-01, -5.7731e-01, -1.3156e-02,  5.8413e-01,\n",
      "          4.2321e-02,  1.4292e-01,  2.3855e-01,  4.7826e-01, -4.6320e-01,\n",
      "         -1.1194e-01, -1.0043e-01,  9.4569e-02,  3.6465e-01, -4.8352e-01,\n",
      "         -6.2386e-01,  7.9158e-01, -6.3180e-01,  5.6919e-01,  1.8001e-01,\n",
      "         -5.3491e-01, -5.0812e-01,  1.0523e-01, -4.6216e-01, -2.7367e-01,\n",
      "          2.1912e-01,  6.5399e-01, -8.3778e-01,  1.5496e-01,  6.5830e-01,\n",
      "         -1.4546e-01,  3.2353e-01,  8.6513e-01,  6.9032e-01,  2.7835e-01,\n",
      "         -6.9143e-01, -8.3950e-01,  7.9349e-01, -6.9802e-02, -8.0402e-01,\n",
      "         -8.5780e-01, -2.5884e-01, -4.2127e-01, -4.1987e-01,  7.4292e-01,\n",
      "          5.1499e-01, -1.1494e-01, -2.5922e-01, -2.6395e-01, -2.3995e-02,\n",
      "          7.3661e-01, -8.7675e-01,  1.2931e-01, -6.9212e-01, -3.5440e-01,\n",
      "         -3.6505e-01, -6.0150e-01, -6.4490e-01, -2.5106e-01,  6.3355e-01,\n",
      "          3.7844e-02, -5.3228e-01,  3.2104e-02,  8.1272e-03,  4.5585e-01,\n",
      "         -8.8697e-02,  9.8031e-02,  4.2087e-01, -4.7306e-01, -8.3092e-02,\n",
      "          3.4941e-01,  5.6153e-01,  5.8594e-01,  6.3711e-01, -4.4502e-03,\n",
      "         -6.5349e-01, -7.7084e-01,  6.9811e-01,  5.2403e-01,  6.7799e-01,\n",
      "         -4.5690e-02, -2.1160e-01,  1.2552e-01, -3.1603e-01,  7.7907e-01,\n",
      "          3.7106e-01,  5.7294e-01, -3.6565e-01, -6.0614e-01,  3.1923e-01,\n",
      "          2.9614e-01, -2.4009e-01, -5.7419e-01, -8.0903e-01,  4.4846e-01,\n",
      "          4.7920e-01, -4.2739e-01,  3.5257e-01, -5.5570e-01, -1.2238e-01,\n",
      "          2.2324e-01, -4.5631e-01, -1.8251e-01, -2.0485e-01,  1.6154e-01,\n",
      "          6.9924e-01,  2.2313e-03, -7.2465e-01, -1.1280e-01, -1.3428e-01,\n",
      "         -3.0247e-01,  2.0755e-01,  2.4401e-01, -1.8444e-01, -9.4448e-03,\n",
      "          7.5806e-01, -1.3059e-01,  3.2281e-02, -6.2696e-02,  2.9975e-01,\n",
      "          1.3559e-01,  1.1189e-02, -8.3400e-02,  1.7419e-01, -4.7510e-01,\n",
      "          1.7255e-01, -5.7498e-01, -2.8710e-01,  3.9121e-01,  7.6555e-01,\n",
      "         -5.7843e-01,  2.2856e-01,  3.2046e-01, -4.4331e-01, -5.4688e-02,\n",
      "         -4.6590e-01,  8.8374e-01, -3.1220e-01,  7.4845e-02,  1.2463e-01,\n",
      "         -1.9672e-01,  8.5654e-01, -1.5555e-02, -5.4100e-01,  7.5422e-01,\n",
      "         -3.2111e-01, -2.3829e-01,  1.1305e-01, -3.8611e-01,  1.8987e-01,\n",
      "          4.5271e-01, -1.0416e-01,  6.5612e-01,  6.3747e-01,  4.9787e-01,\n",
      "         -5.2481e-01, -7.0423e-01, -7.5837e-01,  1.3891e-01, -4.6184e-01,\n",
      "         -3.2305e-01,  2.2852e-01,  4.7269e-01,  4.2302e-01, -8.9998e-01,\n",
      "          1.3088e-01, -5.6865e-01, -8.1943e-01, -6.1109e-01,  4.6380e-01,\n",
      "          5.4334e-01, -3.4755e-01,  1.2173e-01, -3.4099e-01, -3.4894e-01,\n",
      "          5.1746e-01, -4.2880e-01,  3.6366e-01,  8.1520e-01,  4.7164e-01,\n",
      "         -2.4601e-01, -2.7205e-01, -6.1690e-01, -8.5746e-01, -5.4012e-02,\n",
      "          7.7495e-01,  5.7950e-01, -2.0450e-01,  8.2722e-01, -4.0387e-01,\n",
      "          1.4065e-01, -1.3721e-01,  1.7429e-01,  3.0221e-02,  5.9523e-01,\n",
      "          4.3309e-01, -2.3978e-01,  6.4422e-01, -3.1892e-01, -5.3540e-01,\n",
      "          2.2168e-01,  6.4570e-01, -4.1597e-01,  4.3034e-01,  5.7608e-01,\n",
      "         -1.4079e-01,  7.1966e-01, -4.5728e-01,  3.4977e-01,  7.1836e-02,\n",
      "         -8.0319e-01,  8.1955e-03,  2.4779e-01, -6.1310e-02,  7.1715e-02,\n",
      "          5.7502e-01,  1.7871e-02,  7.1226e-01, -4.7410e-01,  2.7650e-02,\n",
      "          3.2474e-01, -2.0614e-01,  5.0672e-01,  2.3201e-01,  5.4053e-01,\n",
      "         -3.4332e-01,  9.0196e-01,  5.1489e-01,  1.9770e-01, -7.7788e-01,\n",
      "          3.9185e-01, -5.3408e-01, -4.7388e-01,  8.5977e-01, -6.3236e-01,\n",
      "          1.7820e-01,  5.5154e-01,  2.5586e-02, -3.4724e-02, -7.5817e-01,\n",
      "         -7.4870e-02, -5.2003e-01,  3.1085e-01,  4.6594e-02, -2.6448e-02,\n",
      "         -5.8477e-01,  2.8585e-01,  1.0698e-01, -3.7320e-01,  4.7655e-01,\n",
      "         -5.0110e-01, -6.1852e-01, -4.0972e-01,  4.5630e-01,  2.4944e-01,\n",
      "         -2.6331e-01,  5.4448e-01, -6.4727e-01,  7.4950e-01,  1.7593e-01,\n",
      "         -1.8962e-01,  5.9970e-01, -4.7405e-01,  1.7470e-01,  7.4021e-01,\n",
      "          3.4677e-01, -2.2019e-01, -5.3869e-01,  1.0665e-01,  2.0523e-01,\n",
      "          3.0865e-01, -3.6000e-01,  6.5473e-03, -5.1017e-01, -7.0241e-01,\n",
      "          1.1612e-01,  6.7573e-01,  4.9310e-01,  7.5214e-02, -3.4056e-01,\n",
      "          8.2284e-02,  6.6241e-01,  1.5570e-02,  6.5935e-02,  3.2431e-02,\n",
      "         -3.1902e-01,  5.1535e-01,  1.9821e-01, -1.5140e-01,  8.4257e-02,\n",
      "          9.5141e-02,  7.7084e-01, -2.7776e-01,  3.4775e-01, -9.2037e-01,\n",
      "          9.4129e-02, -6.3846e-01,  4.1215e-01,  2.0223e-01,  6.9052e-01,\n",
      "          5.1852e-01,  4.7153e-02, -1.8170e-01,  7.2929e-01,  5.6280e-01,\n",
      "         -1.0578e-01,  1.7282e-01, -6.9591e-02,  6.5457e-01,  3.8572e-01,\n",
      "          3.6875e-01,  4.8146e-02, -5.3042e-01, -8.8047e-01, -4.7579e-01,\n",
      "          2.9114e-01,  7.9032e-01, -6.0382e-01, -6.9859e-02, -7.0682e-01,\n",
      "          6.5075e-01, -2.6044e-01,  6.1270e-01, -3.5703e-01,  1.1537e-01,\n",
      "         -4.4715e-01, -4.7926e-02, -3.7943e-01, -4.1442e-01, -3.7067e-01,\n",
      "          5.0673e-01, -3.2536e-01,  4.3768e-01,  9.3990e-03,  1.9989e-02,\n",
      "          1.8476e-01,  3.8819e-01, -2.7160e-01, -1.7610e-01,  2.2682e-01,\n",
      "         -1.3348e-01, -4.8955e-01,  4.7683e-01,  5.2320e-01, -4.2759e-01,\n",
      "          5.4729e-01, -9.6354e-01, -3.5967e-01, -2.3617e-01, -6.3137e-01,\n",
      "         -3.0070e-01,  3.1786e-01, -3.2908e-01,  2.6258e-01, -2.4195e-02,\n",
      "          2.0065e-01, -3.3963e-01,  4.3829e-01,  6.5601e-01,  3.2998e-01,\n",
      "         -2.2135e-01, -3.5978e-01, -2.3675e-01, -6.7702e-01,  6.2709e-03,\n",
      "         -3.2164e-01,  6.9550e-01, -5.9612e-01,  4.5852e-01,  1.9031e-01,\n",
      "         -2.9613e-01, -1.5047e-01, -3.1205e-02, -3.1765e-01,  1.4383e-01,\n",
      "          3.8874e-01, -1.4211e-01,  3.9646e-01,  3.5583e-01,  1.4356e-01,\n",
      "         -1.5093e-01,  6.7604e-01,  1.6707e-01,  5.2277e-02, -1.7343e-01,\n",
      "         -2.2923e-01,  5.3715e-03,  2.4290e-01, -4.8364e-01,  3.5623e-01,\n",
      "          1.0998e-01, -8.2665e-01, -2.9504e-01,  8.1615e-02, -6.8261e-01,\n",
      "          4.2453e-01, -7.5761e-02,  1.0513e-01, -8.0884e-01, -2.3496e-01,\n",
      "         -6.0649e-01, -3.7687e-01, -4.0971e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Configure the BERT model\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertModel(config)\n",
    "\n",
    "# Example of how the model can be used\n",
    "sample_input = df_logs['tokens'].iloc[0]\n",
    "sample_input_tensor = torch.tensor(sample_input).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Pass the input through the model\n",
    "outputs = model(sample_input_tensor)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a85ec3-bebd-425e-a70d-9c472f81cc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nithinrajulapati/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df['tokens'].iloc[idx]), torch.tensor(self.df['label'].iloc[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return inputs_padded, labels\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the structured log file\n",
    "csv_file_path = 'structured_logs.csv'\n",
    "df_logs = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Tokenize the log messages\n",
    "df_logs['tokens'] = df_logs['message'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Assuming we have labels for supervised learning\n",
    "df_logs['label'] = df_logs['message'].apply(lambda x: 1 if 'error' in x.lower() else 0)  # Example labeling\n",
    "dataset = LogDataset(df_logs)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * 3)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Example: 3 epochs\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192639c9-087b-43ad-ae56-9d06b18aa2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ P A D ]\n"
     ]
    }
   ],
   "source": [
    "# Function to decode tokens to human-readable text\n",
    "def decode_tokens(tokens):\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "sample_output_tokens = outputs.logits[0].argmax(dim=-1).detach().numpy()  # Replace with actual output tokens\n",
    "human_readable_output = decode_tokens(sample_output_tokens)\n",
    "print(human_readable_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44e3d04-2de2-4907-91b2-3550a94ee5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nithinrajulapati/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nithinrajulapati/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nithinrajulapati/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming we have a validation set\n",
    "validation_loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)\n",
    "        preds.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "precision = precision_score(true_labels, preds)\n",
    "recall = recall_score(true_labels, preds)\n",
    "f1 = f1_score(true_labels, preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91c04f-425f-468d-b170-9fed8faa52c4",
   "metadata": {},
   "source": [
    "# Handling the issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca008f1-81cc-49d4-868a-ceefe356fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    96\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Labeling logic\n",
    "df_logs['label'] = df_logs['message'].apply(lambda x: 1 if 'error' in x.lower() else 0)\n",
    "\n",
    "# Check label distribution\n",
    "label_counts = df_logs['label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef58d9c8-cb16-4516-bb6a-81105597c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    96\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the label distribution again\n",
    "label_counts = df_logs['label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501984da-623d-4f00-ac5a-5e998ce6b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    68\n",
      "1    28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Adjusted labeling criteria\n",
    "error_keywords = ['deny', 'fail', 'error', 'denied']  # Add more keywords if needed\n",
    "df_logs['label'] = df_logs['message'].apply(lambda x: 1 if any(kw in x.lower() for kw in error_keywords) else 0)\n",
    "\n",
    "# Check the label distribution again\n",
    "label_counts = df_logs['label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5676d37b-d2ac-4adb-a490-c442b78947cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    68\n",
      "1    68\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_logs[df_logs.label == 0]\n",
    "df_minority = df_logs[df_logs.label == 1]\n",
    "\n",
    "# Upsample minority class if it exists\n",
    "if not df_minority.empty:\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,    # sample with replacement\n",
    "                                     n_samples=len(df_majority),  # to match majority class\n",
    "                                     random_state=42)  # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "else:\n",
    "    df_upsampled = df_logs  # If no minority samples, use the original dataframe\n",
    "\n",
    "# Display new class counts\n",
    "print(df_upsampled['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb8ab38-7ae7-43ea-b7d1-3a289a1aeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine dataset and dataloader\n",
    "dataset = LogDataset(df_upsampled)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Assuming a separate validation set is available\n",
    "validation_dataset = LogDataset(df_logs)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b01df8e-34a4-460e-8575-9f0d4a499718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nithinrajulapati/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * 3)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Example: 3 epochs\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b7bb8-4971-41bf-9140-a9b6de05fd4e",
   "metadata": {},
   "source": [
    "# Evaluate the model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a951a7d6-5858-4fae-ac0b-fc7fa0efc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5625\n",
      "Precision: 0.36538461538461536\n",
      "Recall: 0.6785714285714286\n",
      "F1 Score: 0.475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)\n",
    "        preds.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "precision = precision_score(true_labels, preds, zero_division=1)\n",
    "recall = recall_score(true_labels, preds, zero_division=1)\n",
    "f1 = f1_score(true_labels, preds, zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61757436-51fe-4494-9aae-9d1884cabb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Trained Model/tokenizer_config.json',\n",
       " 'Trained Model/special_tokens_map.json',\n",
       " 'Trained Model/vocab.txt',\n",
       " 'Trained Model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `model` is your trained model\n",
    "model_save_path = 'Trained Model'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8780e-3a57-4187-8895-7858e62fc912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2986270-7afa-41a6-8b4a-fa6bbd82b4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
